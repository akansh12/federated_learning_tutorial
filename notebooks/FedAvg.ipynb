{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, num_clients=10, iid=True):\n",
    "    client_datasets = []\n",
    "    \n",
    "    if iid:\n",
    "        # Randomly split dataset into num_clients parts of equal size\n",
    "        client_datasets = random_split(dataset, [len(dataset) // num_clients for _ in range(num_clients)])\n",
    "    else:\n",
    "        # Non-IID case: each client will get a certain subset of the classes\n",
    "        classes_per_client = 2  # Example: each client gets 2 classes of data\n",
    "        labels = dataset.targets\n",
    "        class_indices = {i: torch.where(labels == i)[0] for i in range(10)}\n",
    "        \n",
    "        # Assign classes to each client\n",
    "        for i in range(num_clients):\n",
    "            client_indices = []\n",
    "            for j in range(classes_per_client):\n",
    "                class_id = (i * classes_per_client + j) % 10\n",
    "                client_indices += class_indices[class_id].tolist()\n",
    "            \n",
    "            client_subset = torch.utils.data.Subset(dataset, client_indices)\n",
    "            client_datasets.append(client_subset)\n",
    "\n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_clients = split_dataset(train_dataset, num_clients=10, iid=True)\n",
    "non_iid_clients = split_dataset(train_dataset, num_clients=10, iid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 IID distribution: Counter({np.int64(1): 698, np.int64(7): 644, np.int64(6): 611, np.int64(8): 600, np.int64(3): 598, np.int64(9): 590, np.int64(0): 584, np.int64(2): 567, np.int64(5): 567, np.int64(4): 541})\n",
      "Client 1 IID distribution: Counter({np.int64(1): 670, np.int64(2): 629, np.int64(7): 625, np.int64(9): 622, np.int64(3): 620, np.int64(8): 620, np.int64(0): 573, np.int64(6): 564, np.int64(4): 551, np.int64(5): 526})\n",
      "Client 2 IID distribution: Counter({np.int64(1): 694, np.int64(9): 612, np.int64(2): 607, np.int64(6): 606, np.int64(3): 605, np.int64(4): 603, np.int64(7): 601, np.int64(0): 595, np.int64(8): 544, np.int64(5): 533})\n",
      "Client 3 IID distribution: Counter({np.int64(7): 653, np.int64(1): 647, np.int64(8): 633, np.int64(4): 608, np.int64(3): 608, np.int64(0): 592, np.int64(9): 583, np.int64(6): 574, np.int64(5): 552, np.int64(2): 550})\n",
      "Client 4 IID distribution: Counter({np.int64(1): 698, np.int64(7): 642, np.int64(2): 606, np.int64(9): 598, np.int64(0): 597, np.int64(4): 591, np.int64(6): 587, np.int64(3): 584, np.int64(5): 558, np.int64(8): 539})\n",
      "Client 5 IID distribution: Counter({np.int64(3): 650, np.int64(1): 639, np.int64(7): 618, np.int64(4): 605, np.int64(2): 603, np.int64(6): 595, np.int64(0): 593, np.int64(9): 590, np.int64(8): 569, np.int64(5): 538})\n",
      "Client 6 IID distribution: Counter({np.int64(1): 658, np.int64(2): 643, np.int64(7): 627, np.int64(6): 620, np.int64(9): 596, np.int64(8): 591, np.int64(3): 577, np.int64(0): 575, np.int64(4): 566, np.int64(5): 547})\n",
      "Client 7 IID distribution: Counter({np.int64(1): 642, np.int64(3): 634, np.int64(7): 620, np.int64(2): 616, np.int64(4): 615, np.int64(8): 614, np.int64(6): 571, np.int64(0): 570, np.int64(9): 566, np.int64(5): 552})\n",
      "Client 8 IID distribution: Counter({np.int64(1): 697, np.int64(3): 631, np.int64(0): 624, np.int64(9): 610, np.int64(7): 606, np.int64(6): 590, np.int64(4): 582, np.int64(2): 568, np.int64(8): 565, np.int64(5): 527})\n",
      "Client 9 IID distribution: Counter({np.int64(1): 699, np.int64(7): 629, np.int64(3): 624, np.int64(0): 620, np.int64(6): 600, np.int64(9): 582, np.int64(4): 580, np.int64(8): 576, np.int64(2): 569, np.int64(5): 521})\n",
      "Client 0 Non-IID distribution: Counter({np.int64(1): 6742, np.int64(0): 5923})\n",
      "Client 1 Non-IID distribution: Counter({np.int64(3): 6131, np.int64(2): 5958})\n",
      "Client 2 Non-IID distribution: Counter({np.int64(4): 5842, np.int64(5): 5421})\n",
      "Client 3 Non-IID distribution: Counter({np.int64(7): 6265, np.int64(6): 5918})\n",
      "Client 4 Non-IID distribution: Counter({np.int64(9): 5949, np.int64(8): 5851})\n",
      "Client 5 Non-IID distribution: Counter({np.int64(1): 6742, np.int64(0): 5923})\n",
      "Client 6 Non-IID distribution: Counter({np.int64(3): 6131, np.int64(2): 5958})\n",
      "Client 7 Non-IID distribution: Counter({np.int64(4): 5842, np.int64(5): 5421})\n",
      "Client 8 Non-IID distribution: Counter({np.int64(7): 6265, np.int64(6): 5918})\n",
      "Client 9 Non-IID distribution: Counter({np.int64(9): 5949, np.int64(8): 5851})\n"
     ]
    }
   ],
   "source": [
    "#improve the code for better readability\n",
    "from collections import Counter\n",
    "\n",
    "# Function to count class distribution\n",
    "def class_distribution(dataset):\n",
    "    labels = dataset.dataset.targets[dataset.indices]\n",
    "    return Counter(labels.numpy())\n",
    "\n",
    "# Example: check distribution for IID and Non-IID clients\n",
    "for i, client_data in enumerate(iid_clients):\n",
    "    print(f\"Client {i} IID distribution: {class_distribution(client_data)}\")\n",
    "\n",
    "for i, client_data in enumerate(non_iid_clients):\n",
    "    print(f\"Client {i} Non-IID distribution: {class_distribution(client_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network for MNIST classification\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_training(client_data, model, epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    data_loader = DataLoader(client_data, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_averaging(global_model, client_models):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    \n",
    "    # Initialize weights as 0 for aggregation\n",
    "    for key in global_state_dict.keys():\n",
    "        global_state_dict[key] = torch.zeros_like(global_state_dict[key])\n",
    "    \n",
    "    # Sum the models' parameters\n",
    "    for client_model in client_models:\n",
    "        client_state_dict = client_model.state_dict()\n",
    "        for key in global_state_dict.keys():\n",
    "            global_state_dict[key] += client_state_dict[key]\n",
    "    \n",
    "    # Average the parameters\n",
    "    for key in global_state_dict.keys():\n",
    "        global_state_dict[key] = global_state_dict[key] / len(client_models)\n",
    "    \n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = SimpleNN()\n",
    "\n",
    "client_models = [SimpleNN() for _ in range(10)]\n",
    "for i, client_data in enumerate(iid_clients):\n",
    "    client_models[i].load_state_dict(local_training(client_data, client_models[i]))\n",
    "\n",
    "global_model = federated_averaging(global_model, client_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = test_loss / len(data_loader)\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Model - IID Data | Accuracy: 45.21%, Loss: 1.9212\n"
     ]
    }
   ],
   "source": [
    "iid_accuracy, iid_loss = evaluate_model(global_model, test_loader)\n",
    "print(f\"Global Model - IID Data | Accuracy: {iid_accuracy:.2f}%, Loss: {iid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-IID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Model - Non-IID Data | Accuracy: 16.18%, Loss: 2.2326\n"
     ]
    }
   ],
   "source": [
    "# Example: Train the global model using Non-IID clients\n",
    "client_models_non_iid = [SimpleNN() for _ in range(10)]\n",
    "for i, client_data in enumerate(non_iid_clients):\n",
    "    client_models_non_iid[i].load_state_dict(local_training(client_data, client_models_non_iid[i]))\n",
    "\n",
    "global_model_non_iid = SimpleNN()\n",
    "global_model_non_iid = federated_averaging(global_model_non_iid, client_models_non_iid)\n",
    "\n",
    "# Evaluate the global model trained on Non-IID data\n",
    "non_iid_accuracy, non_iid_loss = evaluate_model(global_model_non_iid, test_loader)\n",
    "print(f\"Global Model - Non-IID Data | Accuracy: {non_iid_accuracy:.2f}%, Loss: {non_iid_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
